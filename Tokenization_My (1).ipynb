{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Character level tokenization**"
      ],
      "metadata": {
        "id": "cvwqbDTduNqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the Shakespear data set. You can download it from https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\n",
        "\n",
        "#read the text file\n",
        "with open('Shakespear.txt','r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "#create a sorted character set\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "\n",
        "# create a mapping from the characters to integers\n",
        "stoi = {ch : i for i , ch in enumerate(chars)}\n",
        "itos = {i : ch for i , ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s : [stoi[c] for c in s] #encoder: take a string, output a list of numbers\n",
        "decode = lambda l : ''.join([itos[i] for i in l]) #decoder: take a list of numbers, output the string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxehswEQuU2i",
        "outputId": "57388899-8729-4e15-ecb8-055f462434a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"Hi there!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6dX1dktu3s8",
        "outputId": "6e2ac9fe-7ab6-4ab2-b0a9-5f43da16de14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17, 44, 1, 55, 43, 40, 53, 40, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"안녕하세요 👋 (hello in Korean!)\"\n",
        "\n",
        "print([ord(x) for x in string])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3US5x8tJEqWF",
        "outputId": "f408e7ce-d509-4555-a466-30df544a0af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50504, 45397, 54616, 49464, 50836, 32, 128075, 32, 40, 104, 101, 108, 108, 111, 32, 105, 110, 32, 75, 111, 114, 101, 97, 110, 33, 41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(\"안녕하세요 👋 (hello in Korean!)\".encode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUa_o4VfHU-M",
        "outputId": "c7ce3426-eb4f-48d4-f1c3-4415cb13c8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[236,\n",
              " 149,\n",
              " 136,\n",
              " 235,\n",
              " 133,\n",
              " 149,\n",
              " 237,\n",
              " 149,\n",
              " 152,\n",
              " 236,\n",
              " 132,\n",
              " 184,\n",
              " 236,\n",
              " 154,\n",
              " 148,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 145,\n",
              " 139,\n",
              " 32,\n",
              " 40,\n",
              " 104,\n",
              " 101,\n",
              " 108,\n",
              " 108,\n",
              " 111,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 75,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 33,\n",
              " 41]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Byte pair encoding**"
      ],
      "metadata": {
        "id": "FGImT8ie81mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "print(text)\n",
        "print(\"length:\", len(text))\n",
        "print('---')\n",
        "print(tokens)\n",
        "print(\"length:\", len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQxIEkyzw5P9",
        "outputId": "884a2eb0-20d8-44b2-b3ca-c1a38839fd34"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
            "length: 533\n",
            "---\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "length: 616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "counter = 0\n",
        "\n",
        "while counter < 10:\n",
        "  # create a dictionary of token pairs and their counts\n",
        "  tokenPairCounts = dict()\n",
        "  for i in range(len(tokens)-1):\n",
        "    pair = (tokens[i], tokens[i+1])\n",
        "    if pair in tokenPairCounts:\n",
        "      tokenPairCounts[pair] += 1\n",
        "    else:\n",
        "      tokenPairCounts[pair] = 1\n",
        "\n",
        "  # order the tokenPairCounts dict based on the value. Get the key with the maximum value\n",
        "  max_key = max(tokenPairCounts, key=tokenPairCounts.get)\n",
        "  print(max_key)\n",
        "\n",
        "  #traverse through the tokens list and replace the max_key with a new token\n",
        "  i = 0\n",
        "  while i < len(tokens)-1:\n",
        "    pair = (tokens[i], tokens[i+1])\n",
        "    if pair == max_key:\n",
        "      tokens[i] = 255 + counter\n",
        "      del tokens[i+1]\n",
        "    i += 1\n",
        "  counter +=1\n",
        "\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axkD5G-R18tj",
        "outputId": "feab3717-30e9-4a42-e025-dbc589271a7d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 32)\n",
            "(240, 159)\n",
            "(226, 128)\n",
            "(105, 110)\n",
            "(115, 32)\n",
            "(97, 110)\n",
            "(116, 104)\n",
            "(256, 133)\n",
            "(256, 135)\n",
            "(97, 114)\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 262, 164, 262, 157, 262, 152, 262, 146, 262, 158, 262, 147, 262, 148, 257, 189, 32, 263, 186, 257, 140, 263, 179, 257, 140, 263, 174, 257, 140, 263, 168, 257, 140, 263, 180, 257, 140, 263, 169, 257, 140, 263, 170, 33, 32, 256, 152, 132, 32, 84, 104, 255, 118, 101, 114, 121, 32, 110, 97, 109, 255, 115, 116, 114, 105, 107, 101, 259, 102, 101, 264, 32, 260, 100, 32, 97, 119, 255, 258, 116, 111, 32, 261, 255, 104, 101, 264, 116, 259, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 259, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 255, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 255, 111, 117, 103, 104, 116, 32, 116, 111, 32, 257, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 257, 157, 32, 258, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 264, 255, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 261, 97, 116, 32, 109, 101, 260, 115, 257, 148, 108, 105, 107, 255, 117, 115, 258, 103, 32, 119, 99, 104, 264, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 261, 255, 115, 116, 114, 258, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 255, 99, 260, 32, 98, 255, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 260, 100, 32, 100, 105, 118, 258, 103, 32, 258, 116, 111, 32, 261, 255, 261, 111, 117, 115, 260, 100, 45, 112, 97, 103, 255, 85, 110, 105, 99, 111, 100, 255, 83, 116, 260, 100, 264, 100, 32, 112, 108, 117, 259, 105, 116, 259, 100, 111, 122, 101, 110, 259, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 264, 121, 32, 260, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 260, 100, 32, 110, 111, 116, 101, 259, 99, 260, 32, 98, 255, 109, 111, 114, 255, 261, 260, 32, 97, 32, 108, 105, 116, 116, 108, 255, 258, 116, 105, 109, 105, 100, 97, 116, 258, 103, 46, 32, 73, 32, 100, 111, 110, 257, 153, 116, 32, 98, 108, 97, 109, 255, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 259, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 258, 100, 258, 103, 32, 261, 255, 119, 104, 111, 108, 255, 261, 258, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 264, 259, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 257, 153, 259, 258, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the consecutive pairs of tokens\n",
        "def get_stats(ids):\n",
        "  counts = {}\n",
        "  for pair in zip(ids,ids[1:]): # create pairs of tokens\n",
        "    counts[pair] = counts.get(pair,0) + 1\n",
        "  return counts\n",
        "\n",
        "print(get_stats(tokens))\n",
        "\n",
        "# we van reverse the key value pairs and sort the dictionary based on the values\n",
        "# stats.item() gives you (key,value) tuples\n",
        "\n",
        "stats = get_stats(tokens)\n",
        "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDjdf5Vx9dut",
        "outputId": "1ddd1ed1-de94-4145-bce7-dc7cafb6b736"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(239, 188): 1, (188, 181): 1, (181, 239): 1, (239, 189): 6, (189, 142): 1, (142, 239): 1, (189, 137): 1, (137, 239): 1, (189, 131): 1, (131, 239): 1, (189, 143): 1, (143, 239): 1, (189, 132): 1, (132, 239): 1, (189, 133): 1, (133, 33): 1, (33, 32): 2, (32, 240): 3, (240, 159): 15, (159, 133): 7, (133, 164): 1, (164, 240): 1, (133, 157): 1, (157, 240): 1, (133, 152): 1, (152, 240): 1, (133, 146): 1, (146, 240): 1, (133, 158): 1, (158, 240): 1, (133, 147): 1, (147, 240): 1, (133, 148): 1, (148, 226): 1, (226, 128): 12, (128, 189): 1, (189, 32): 1, (159, 135): 7, (135, 186): 1, (186, 226): 1, (128, 140): 6, (140, 240): 6, (135, 179): 1, (179, 226): 1, (135, 174): 1, (174, 226): 1, (135, 168): 1, (168, 226): 1, (135, 180): 1, (180, 226): 1, (135, 169): 1, (169, 226): 1, (135, 170): 1, (170, 33): 1, (159, 152): 1, (152, 132): 1, (132, 32): 1, (32, 84): 1, (84, 104): 1, (104, 101): 6, (101, 32): 20, (32, 118): 1, (118, 101): 3, (101, 114): 6, (114, 121): 2, (121, 32): 2, (32, 110): 2, (110, 97): 1, (97, 109): 4, (109, 101): 6, (32, 115): 5, (115, 116): 5, (116, 114): 3, (114, 105): 4, (105, 107): 2, (107, 101): 2, (101, 115): 3, (115, 32): 10, (32, 102): 4, (102, 101): 1, (101, 97): 4, (97, 114): 7, (114, 32): 6, (32, 97): 10, (97, 110): 10, (110, 100): 6, (100, 32): 4, (97, 119): 1, (119, 101): 2, (32, 105): 6, (105, 110): 12, (110, 116): 4, (116, 111): 3, (111, 32): 3, (32, 116): 9, (116, 104): 8, (32, 104): 1, (114, 116): 3, (116, 115): 3, (32, 111): 4, (111, 102): 3, (102, 32): 2, (32, 112): 3, (112, 114): 2, (114, 111): 2, (111, 103): 2, (103, 114): 2, (114, 97): 2, (109, 109): 2, (114, 115): 3, (32, 119): 4, (119, 111): 1, (111, 114): 6, (114, 108): 1, (108, 100): 1, (100, 119): 1, (119, 105): 1, (105, 100): 2, (100, 101): 5, (101, 46): 1, (46, 32): 3, (32, 87): 1, (87, 101): 1, (97, 108): 2, (108, 108): 3, (108, 32): 3, (32, 107): 1, (107, 110): 1, (110, 111): 2, (111, 119): 1, (119, 32): 1, (111, 117): 4, (117, 103): 1, (103, 104): 2, (104, 116): 2, (116, 32): 6, (32, 226): 1, (128, 156): 1, (156, 115): 1, (115, 117): 2, (117, 112): 2, (112, 112): 2, (112, 111): 2, (32, 85): 4, (85, 110): 4, (110, 105): 4, (105, 99): 4, (99, 111): 4, (111, 100): 4, (101, 226): 2, (128, 157): 1, (157, 32): 1, (110, 32): 5, (117, 114): 1, (115, 111): 1, (102, 116): 2, (116, 119): 1, (119, 97): 1, (114, 101): 3, (32, 40): 1, (40, 119): 1, (119, 104): 2, (104, 97): 4, (97, 116): 3, (116, 101): 4, (101, 118): 2, (32, 109): 3, (110, 115): 2, (115, 226): 1, (128, 148): 1, (148, 108): 1, (108, 105): 2, (32, 117): 1, (117, 115): 5, (115, 105): 1, (110, 103): 6, (103, 32): 4, (119, 99): 1, (99, 104): 1, (114, 95): 1, (95, 116): 1, (102, 111): 2, (103, 115): 1, (115, 44): 4, (44, 32): 5, (32, 114): 2, (105, 103): 1, (116, 63): 1, (63, 41): 1, (41, 46): 1, (32, 66): 1, (66, 117): 1, (117, 116): 1, (32, 99): 2, (99, 97): 2, (32, 98): 3, (98, 101): 2, (97, 98): 1, (98, 115): 1, (114, 117): 1, (115, 101): 1, (101, 44): 1, (32, 100): 3, (100, 105): 2, (105, 118): 1, (118, 105): 1, (104, 111): 2, (115, 97): 1, (100, 45): 1, (45, 112): 1, (112, 97): 1, (97, 103): 1, (103, 101): 1, (32, 83): 1, (83, 116): 1, (116, 97): 2, (100, 97): 2, (114, 100): 1, (112, 108): 2, (108, 117): 1, (105, 116): 2, (100, 111): 2, (111, 122): 1, (122, 101): 1, (101, 110): 3, (108, 101): 3, (101, 109): 1, (110, 110): 1, (110, 101): 1, (101, 120): 1, (120, 101): 1, (101, 112): 2, (111, 116): 1, (109, 111): 1, (97, 32): 1, (32, 108): 1, (116, 116): 1, (116, 108): 1, (116, 105): 4, (105, 109): 1, (109, 105): 1, (103, 46): 1, (32, 73): 1, (73, 32): 1, (111, 110): 2, (110, 226): 1, (128, 153): 2, (153, 116): 1, (98, 108): 1, (108, 97): 1, (105, 108): 1, (102, 105): 1, (111, 108): 1, (104, 105): 1, (109, 121): 1, (121, 115): 1, (105, 111): 2, (32, 101): 1, (32, 51): 1, (51, 48): 1, (48, 32): 1, (32, 121): 1, (121, 101): 1, (97, 102): 1, (153, 115): 1, (110, 99): 1, (99, 101): 1, (112, 116): 1, (110, 46): 1}\n",
            "[(20, (101, 32)), (15, (240, 159)), (12, (226, 128)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (116, 104)), (7, (159, 135)), (7, (159, 133)), (7, (97, 114)), (6, (239, 189)), (6, (140, 240)), (6, (128, 140)), (6, (116, 32)), (6, (114, 32)), (6, (111, 114)), (6, (110, 103)), (6, (110, 100)), (6, (109, 101)), (6, (104, 101)), (6, (101, 114)), (6, (32, 105)), (5, (117, 115)), (5, (115, 116)), (5, (110, 32)), (5, (100, 101)), (5, (44, 32)), (5, (32, 115)), (4, (116, 105)), (4, (116, 101)), (4, (115, 44)), (4, (114, 105)), (4, (111, 117)), (4, (111, 100)), (4, (110, 116)), (4, (110, 105)), (4, (105, 99)), (4, (104, 97)), (4, (103, 32)), (4, (101, 97)), (4, (100, 32)), (4, (99, 111)), (4, (97, 109)), (4, (85, 110)), (4, (32, 119)), (4, (32, 111)), (4, (32, 102)), (4, (32, 85)), (3, (118, 101)), (3, (116, 115)), (3, (116, 114)), (3, (116, 111)), (3, (114, 116)), (3, (114, 115)), (3, (114, 101)), (3, (111, 102)), (3, (111, 32)), (3, (108, 108)), (3, (108, 101)), (3, (108, 32)), (3, (101, 115)), (3, (101, 110)), (3, (97, 116)), (3, (46, 32)), (3, (32, 240)), (3, (32, 112)), (3, (32, 109)), (3, (32, 100)), (3, (32, 98)), (2, (128, 153)), (2, (121, 32)), (2, (119, 104)), (2, (119, 101)), (2, (117, 112)), (2, (116, 97)), (2, (115, 117)), (2, (114, 121)), (2, (114, 111)), (2, (114, 97)), (2, (112, 114)), (2, (112, 112)), (2, (112, 111)), (2, (112, 108)), (2, (111, 110)), (2, (111, 103)), (2, (110, 115)), (2, (110, 111)), (2, (109, 109)), (2, (108, 105)), (2, (107, 101)), (2, (105, 116)), (2, (105, 111)), (2, (105, 107)), (2, (105, 100)), (2, (104, 116)), (2, (104, 111)), (2, (103, 114)), (2, (103, 104)), (2, (102, 116)), (2, (102, 111)), (2, (102, 32)), (2, (101, 226)), (2, (101, 118)), (2, (101, 112)), (2, (100, 111)), (2, (100, 105)), (2, (100, 97)), (2, (99, 97)), (2, (98, 101)), (2, (97, 108)), (2, (33, 32)), (2, (32, 114)), (2, (32, 110)), (2, (32, 99)), (1, (239, 188)), (1, (189, 143)), (1, (189, 142)), (1, (189, 137)), (1, (189, 133)), (1, (189, 132)), (1, (189, 131)), (1, (189, 32)), (1, (188, 181)), (1, (186, 226)), (1, (181, 239)), (1, (180, 226)), (1, (179, 226)), (1, (174, 226)), (1, (170, 33)), (1, (169, 226)), (1, (168, 226)), (1, (164, 240)), (1, (159, 152)), (1, (158, 240)), (1, (157, 240)), (1, (157, 32)), (1, (156, 115)), (1, (153, 116)), (1, (153, 115)), (1, (152, 240)), (1, (152, 132)), (1, (148, 226)), (1, (148, 108)), (1, (147, 240)), (1, (146, 240)), (1, (143, 239)), (1, (142, 239)), (1, (137, 239)), (1, (135, 186)), (1, (135, 180)), (1, (135, 179)), (1, (135, 174)), (1, (135, 170)), (1, (135, 169)), (1, (135, 168)), (1, (133, 164)), (1, (133, 158)), (1, (133, 157)), (1, (133, 152)), (1, (133, 148)), (1, (133, 147)), (1, (133, 146)), (1, (133, 33)), (1, (132, 239)), (1, (132, 32)), (1, (131, 239)), (1, (128, 189)), (1, (128, 157)), (1, (128, 156)), (1, (128, 148)), (1, (122, 101)), (1, (121, 115)), (1, (121, 101)), (1, (120, 101)), (1, (119, 111)), (1, (119, 105)), (1, (119, 99)), (1, (119, 97)), (1, (119, 32)), (1, (118, 105)), (1, (117, 116)), (1, (117, 114)), (1, (117, 103)), (1, (116, 119)), (1, (116, 116)), (1, (116, 108)), (1, (116, 63)), (1, (115, 226)), (1, (115, 111)), (1, (115, 105)), (1, (115, 101)), (1, (115, 97)), (1, (114, 117)), (1, (114, 108)), (1, (114, 100)), (1, (114, 95)), (1, (112, 116)), (1, (112, 97)), (1, (111, 122)), (1, (111, 119)), (1, (111, 116)), (1, (111, 108)), (1, (110, 226)), (1, (110, 110)), (1, (110, 101)), (1, (110, 99)), (1, (110, 97)), (1, (110, 46)), (1, (109, 121)), (1, (109, 111)), (1, (109, 105)), (1, (108, 117)), (1, (108, 100)), (1, (108, 97)), (1, (107, 110)), (1, (105, 118)), (1, (105, 109)), (1, (105, 108)), (1, (105, 103)), (1, (104, 105)), (1, (103, 115)), (1, (103, 101)), (1, (103, 46)), (1, (102, 105)), (1, (102, 101)), (1, (101, 120)), (1, (101, 109)), (1, (101, 46)), (1, (101, 44)), (1, (100, 119)), (1, (100, 45)), (1, (99, 104)), (1, (99, 101)), (1, (98, 115)), (1, (98, 108)), (1, (97, 119)), (1, (97, 103)), (1, (97, 102)), (1, (97, 98)), (1, (97, 32)), (1, (95, 116)), (1, (87, 101)), (1, (84, 104)), (1, (83, 116)), (1, (73, 32)), (1, (66, 117)), (1, (63, 41)), (1, (51, 48)), (1, (48, 32)), (1, (45, 112)), (1, (41, 46)), (1, (40, 119)), (1, (32, 226)), (1, (32, 121)), (1, (32, 118)), (1, (32, 117)), (1, (32, 108)), (1, (32, 107)), (1, (32, 104)), (1, (32, 101)), (1, (32, 87)), (1, (32, 84)), (1, (32, 83)), (1, (32, 73)), (1, (32, 66)), (1, (32, 51)), (1, (32, 40))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the most common token pair\n",
        "top_pair = max(stats, key = stats.get)\n",
        "print(top_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFpl_qRHdFWc",
        "outputId": "c5158198-b4e5-41f6-8109-eaffd8437de6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(ids, pair, idx):\n",
        "  # traverse through the ids list and replace the pair with idx\n",
        "  i = 0\n",
        "  newids = []\n",
        "  while i < len(ids):\n",
        "    #make sure the index do not go out of bound\n",
        "    if i<len(ids)-1 and (ids[i], ids[i+1]) == pair:\n",
        "      newids.append(idx)\n",
        "      i +=2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i +=1\n",
        "  return newids\n",
        "\n",
        "print(top_pair)\n",
        "newTokens = merge(tokens, top_pair, 256)\n",
        "print(newTokens)\n",
        "print(len(newTokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvzG-EA9eDME",
        "outputId": "9b15d190-4b5c-4306-8686-5adb1cab06d0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 32)\n",
            "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
            "596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 276\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) #create a copy of the original token list\n",
        "\n",
        "merges = {}\n",
        "\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key = stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into new token {idx}\")\n",
        "  merges[pair] = idx\n",
        "  ids = merge(ids, pair, idx)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of1qS1rjkvye",
        "outputId": "09bf61cc-e0cf-4d8e-aeb7-7d3bda5555e7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into new token 256\n",
            "merging (240, 159) into new token 257\n",
            "merging (226, 128) into new token 258\n",
            "merging (105, 110) into new token 259\n",
            "merging (115, 32) into new token 260\n",
            "merging (97, 110) into new token 261\n",
            "merging (116, 104) into new token 262\n",
            "merging (257, 133) into new token 263\n",
            "merging (257, 135) into new token 264\n",
            "merging (97, 114) into new token 265\n",
            "merging (239, 189) into new token 266\n",
            "merging (258, 140) into new token 267\n",
            "merging (267, 264) into new token 268\n",
            "merging (101, 114) into new token 269\n",
            "merging (111, 114) into new token 270\n",
            "merging (116, 32) into new token 271\n",
            "merging (259, 103) into new token 272\n",
            "merging (115, 116) into new token 273\n",
            "merging (261, 100) into new token 274\n",
            "merging (32, 262) into new token 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"token length: {len(tokens)}\")\n",
        "print(f\"ids length: {len(ids)}\")\n",
        "print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVgFq2E1nhIk",
        "outputId": "95bb0e9f-7fc8-40da-8b0e-d486925842b3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token length: 616\n",
            "ids length: 451\n",
            "compression ratio: 1.37X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decoder design**"
      ],
      "metadata": {
        "id": "MyCe2ybtuzmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(ids):\n",
        "  '''given a list of integers (ids), output a string'''\n",
        "\n",
        "  # reverse key value pairs of the merges dictionary\n",
        "  # this helps us to go from the root to the leaves\n",
        "  reverse_merges = {v:k for k,v in merges.items()}\n",
        "\n",
        "  for i in range(num_merges):\n",
        "    new_ids =[]\n",
        "    for item in ids:\n",
        "      if item in reverse_merges:\n",
        "        new_ids.extend(list(reverse_merges[item]))\n",
        "      else:\n",
        "        new_ids.append(item)\n",
        "    ids = new_ids\n",
        "\n",
        "  # convert the integres back to the characters\n",
        "  char_list = []\n",
        "  for j in ids:\n",
        "    char_list.append(chr(j))\n",
        "  return \"\".join(char_list)\n"
      ],
      "metadata": {
        "id": "5O-qIIwau3_j"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "580dKeOHwlMY",
        "outputId": "984eed3b-9819-41b9-e3ed-7a24980717cc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï¼µï½ï½ï½ï½ï½ï½! ð¤ððððððâ½ ðºâð³âð®âð¨âð´âð©âðª! ð The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to âsupport Unicodeâ in our software (whatever that meansâlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donât blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâs inception.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A mapping from integers to the bytes\n",
        "vocab = {idx: bytes(idx) for idx in range(256)}\n",
        "for (p0,p1),idx in merges.items():\n",
        "  # bytes addition just concatenates them\n",
        "  vocab[idx] = vocab[p0] + vocab[p1]"
      ],
      "metadata": {
        "id": "KaMP0xTsz-jh"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(ids):\n",
        "  '''given a list of integers (ids), output a string'''\n",
        "\n",
        "  # concatenate the bytes\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  # decode the byte sequence\n",
        "  text = tokens.decode(\"utf-8\",errors='replace')\n",
        "  return text\n",
        "\n",
        "print(decoder([128]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdjZW7rX1Md7",
        "outputId": "e1e4fdbf-5021-44dc-b6f1-2dd9e7ca47a0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  # given ids (list of integers), return Python string\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "print(decode([128]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4c29JdV2h0k",
        "outputId": "83ab52ab-16a4-4173-b996-c59a6bf86302"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bin(128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H38vA7jV3eRW",
        "outputId": "bbea042a-2edc-42b5-afa4-93b6dfc1cba2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0b10000000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder design**"
      ],
      "metadata": {
        "id": "4uHLw2Kt5QtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(text):\n",
        "  '''given a string, output a list of integers (ids)'''\n",
        "  tokens = text.encode(\"utf-8\")\n",
        "  ids = list(map(int, tokens))\n",
        "\n",
        "  vocab_size = 276\n",
        "  num_merges = vocab_size - 256\n",
        "\n",
        "  for i in range(num_merges):\n",
        "    stats = get_stats(ids)\n",
        "    #pair = max(stats, key = stats.get)\n",
        "    idx = 256 + i\n",
        "    ids = merge(ids, pair, idx)\n",
        "  return ids\n",
        "\n",
        "print(encoder(\"Hi there!\"))"
      ],
      "metadata": {
        "id": "G7Ga31RG5VxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_new(text):\n",
        "  '''given a string, output a list of integers (ids)'''\n",
        "\n",
        "  # Get a list of raw bytes\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "\n",
        "  while len(tokens)>=2: # when the text is a single character or an empty string, the stats is empty\n",
        "    # get the set of all pairs\n",
        "    stats = get_stats(tokens)\n",
        "\n",
        "    # get the pair with the least index\n",
        "    pair = min(stats, key= lambda p: stats.get(p, float('inf')))\n",
        "\n",
        "    # if there are no more merging pairs, break the loop\n",
        "    if pair not in merges:\n",
        "      break\n",
        "\n",
        "    # merge the pair\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "print(encoder_new(\"Hi there!\"))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV2C_oI9BKh6",
        "outputId": "be8c35aa-7064-4c26-a484-093e73b237c6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 105, 32, 116, 104, 101, 114, 101, 33]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Regex**"
      ],
      "metadata": {
        "id": "4OEyilXSZ6I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "\n",
        "# create the regex pattern\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "\n",
        "# match the pattern aginst the sequence left to right\n",
        "print(re.findall(gpt2pat, \"Hello've world123 how's are you!!!?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o02akRNHayPX",
        "outputId": "c12a446f-97b1-436b-c96e-62264ca30d36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **tiktoken**"
      ],
      "metadata": {
        "id": "Z_b_8wo9Z_xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98Qfe7Hyqh7S",
        "outputId": "d4cbbb6c-387d-4bac-8f4d-bc321eeb0621"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "#GPT-2 (spaces does not merge)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"    Hello world\"))\n",
        "\n",
        "#GPT-4 (spaces merge)\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"    Hello world\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdZBrGfIqvoO",
        "outputId": "aec0c955-cbbe-4ba8-8dd7-3a6ccb7e6414"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 220, 18435, 995]\n",
            "[262, 22691, 1917]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt4pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")"
      ],
      "metadata": {
        "id": "G2AD1N4Lt9P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
        "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe1T1qXgySQJ",
        "outputId": "5e2db765-e0fb-4d4c-f61a-8fede7118970"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-25 09:06:13--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 456318 (446K) [application/octet-stream]\n",
            "Saving to: ‘vocab.bpe’\n",
            "\n",
            "vocab.bpe           100%[===================>] 445.62K  1.34MB/s    in 0.3s    \n",
            "\n",
            "2024-08-25 09:06:13 (1.34 MB/s) - ‘vocab.bpe’ saved [456318/456318]\n",
            "\n",
            "--2024-08-25 09:06:13--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
            "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
            "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1042301 (1018K) [application/json]\n",
            "Saving to: ‘encoder.json’\n",
            "\n",
            "encoder.json        100%[===================>]   1018K  2.64MB/s    in 0.4s    \n",
            "\n",
            "2024-08-25 09:06:14 (2.64 MB/s) - ‘encoder.json’ saved [1042301/1042301]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "\n",
        "with open('encoder.json', 'r') as f:\n",
        "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
        "\n",
        "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
        "    bpe_data = f.read()\n",
        "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]"
      ],
      "metadata": {
        "id": "O8vzNY0DyVDM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentencepiece**"
      ],
      "metadata": {
        "id": "klVs94Wej4tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# write a toy.txt file with some random text\n",
        "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
      ],
      "metadata": {
        "id": "wW_WlkEdYro0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a sentencepiece model on it\n",
        "# the settings here are (best effort) those used for training Llama 2\n",
        "import os\n",
        "\n",
        "options = dict(\n",
        "  # input spec\n",
        "  input=\"toy.txt\",\n",
        "  input_format=\"text\",\n",
        "  # output spec\n",
        "  model_prefix=\"tok400\", # output filename prefix\n",
        "  # algorithm spec\n",
        "  # BPE alg\n",
        "  model_type=\"bpe\",\n",
        "  vocab_size=400,\n",
        "  # normalization\n",
        "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
        "  remove_extra_whitespaces=False,\n",
        "  input_sentence_size=200000000, # max number of training sentences\n",
        "  max_sentence_length=4192, # max number of bytes per sentence\n",
        "  seed_sentencepiece_size=1000000,\n",
        "  shuffle_input_sentence=True,\n",
        "  # rare word treatment\n",
        "  character_coverage=0.99995,\n",
        "  byte_fallback=True,\n",
        "  # merge rules\n",
        "  split_digits=True,\n",
        "  split_by_unicode_script=True,\n",
        "  split_by_whitespace=True,\n",
        "  split_by_number=True,\n",
        "  max_sentencepiece_length=16,\n",
        "  add_dummy_prefix=True,\n",
        "  allow_whitespace_only_pieces=True,\n",
        "  # special tokens\n",
        "  unk_id=0, # the UNK token MUST exist\n",
        "  bos_id=1, # the others are optional, set to -1 to turn off\n",
        "  eos_id=2,\n",
        "  pad_id=-1,\n",
        "  # systems\n",
        "  num_threads=os.cpu_count(), # use ~all system resources\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.train(**options)"
      ],
      "metadata": {
        "id": "8obpZKSuYtfj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('tok400.model')\n",
        "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xswdQS-kUrf",
        "outputId": "39421f61-ec71-4c17-cd5d-cb53d65f0ed0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<unk>', 0],\n",
              " ['<s>', 1],\n",
              " ['</s>', 2],\n",
              " ['<0x00>', 3],\n",
              " ['<0x01>', 4],\n",
              " ['<0x02>', 5],\n",
              " ['<0x03>', 6],\n",
              " ['<0x04>', 7],\n",
              " ['<0x05>', 8],\n",
              " ['<0x06>', 9],\n",
              " ['<0x07>', 10],\n",
              " ['<0x08>', 11],\n",
              " ['<0x09>', 12],\n",
              " ['<0x0A>', 13],\n",
              " ['<0x0B>', 14],\n",
              " ['<0x0C>', 15],\n",
              " ['<0x0D>', 16],\n",
              " ['<0x0E>', 17],\n",
              " ['<0x0F>', 18],\n",
              " ['<0x10>', 19],\n",
              " ['<0x11>', 20],\n",
              " ['<0x12>', 21],\n",
              " ['<0x13>', 22],\n",
              " ['<0x14>', 23],\n",
              " ['<0x15>', 24],\n",
              " ['<0x16>', 25],\n",
              " ['<0x17>', 26],\n",
              " ['<0x18>', 27],\n",
              " ['<0x19>', 28],\n",
              " ['<0x1A>', 29],\n",
              " ['<0x1B>', 30],\n",
              " ['<0x1C>', 31],\n",
              " ['<0x1D>', 32],\n",
              " ['<0x1E>', 33],\n",
              " ['<0x1F>', 34],\n",
              " ['<0x20>', 35],\n",
              " ['<0x21>', 36],\n",
              " ['<0x22>', 37],\n",
              " ['<0x23>', 38],\n",
              " ['<0x24>', 39],\n",
              " ['<0x25>', 40],\n",
              " ['<0x26>', 41],\n",
              " ['<0x27>', 42],\n",
              " ['<0x28>', 43],\n",
              " ['<0x29>', 44],\n",
              " ['<0x2A>', 45],\n",
              " ['<0x2B>', 46],\n",
              " ['<0x2C>', 47],\n",
              " ['<0x2D>', 48],\n",
              " ['<0x2E>', 49],\n",
              " ['<0x2F>', 50],\n",
              " ['<0x30>', 51],\n",
              " ['<0x31>', 52],\n",
              " ['<0x32>', 53],\n",
              " ['<0x33>', 54],\n",
              " ['<0x34>', 55],\n",
              " ['<0x35>', 56],\n",
              " ['<0x36>', 57],\n",
              " ['<0x37>', 58],\n",
              " ['<0x38>', 59],\n",
              " ['<0x39>', 60],\n",
              " ['<0x3A>', 61],\n",
              " ['<0x3B>', 62],\n",
              " ['<0x3C>', 63],\n",
              " ['<0x3D>', 64],\n",
              " ['<0x3E>', 65],\n",
              " ['<0x3F>', 66],\n",
              " ['<0x40>', 67],\n",
              " ['<0x41>', 68],\n",
              " ['<0x42>', 69],\n",
              " ['<0x43>', 70],\n",
              " ['<0x44>', 71],\n",
              " ['<0x45>', 72],\n",
              " ['<0x46>', 73],\n",
              " ['<0x47>', 74],\n",
              " ['<0x48>', 75],\n",
              " ['<0x49>', 76],\n",
              " ['<0x4A>', 77],\n",
              " ['<0x4B>', 78],\n",
              " ['<0x4C>', 79],\n",
              " ['<0x4D>', 80],\n",
              " ['<0x4E>', 81],\n",
              " ['<0x4F>', 82],\n",
              " ['<0x50>', 83],\n",
              " ['<0x51>', 84],\n",
              " ['<0x52>', 85],\n",
              " ['<0x53>', 86],\n",
              " ['<0x54>', 87],\n",
              " ['<0x55>', 88],\n",
              " ['<0x56>', 89],\n",
              " ['<0x57>', 90],\n",
              " ['<0x58>', 91],\n",
              " ['<0x59>', 92],\n",
              " ['<0x5A>', 93],\n",
              " ['<0x5B>', 94],\n",
              " ['<0x5C>', 95],\n",
              " ['<0x5D>', 96],\n",
              " ['<0x5E>', 97],\n",
              " ['<0x5F>', 98],\n",
              " ['<0x60>', 99],\n",
              " ['<0x61>', 100],\n",
              " ['<0x62>', 101],\n",
              " ['<0x63>', 102],\n",
              " ['<0x64>', 103],\n",
              " ['<0x65>', 104],\n",
              " ['<0x66>', 105],\n",
              " ['<0x67>', 106],\n",
              " ['<0x68>', 107],\n",
              " ['<0x69>', 108],\n",
              " ['<0x6A>', 109],\n",
              " ['<0x6B>', 110],\n",
              " ['<0x6C>', 111],\n",
              " ['<0x6D>', 112],\n",
              " ['<0x6E>', 113],\n",
              " ['<0x6F>', 114],\n",
              " ['<0x70>', 115],\n",
              " ['<0x71>', 116],\n",
              " ['<0x72>', 117],\n",
              " ['<0x73>', 118],\n",
              " ['<0x74>', 119],\n",
              " ['<0x75>', 120],\n",
              " ['<0x76>', 121],\n",
              " ['<0x77>', 122],\n",
              " ['<0x78>', 123],\n",
              " ['<0x79>', 124],\n",
              " ['<0x7A>', 125],\n",
              " ['<0x7B>', 126],\n",
              " ['<0x7C>', 127],\n",
              " ['<0x7D>', 128],\n",
              " ['<0x7E>', 129],\n",
              " ['<0x7F>', 130],\n",
              " ['<0x80>', 131],\n",
              " ['<0x81>', 132],\n",
              " ['<0x82>', 133],\n",
              " ['<0x83>', 134],\n",
              " ['<0x84>', 135],\n",
              " ['<0x85>', 136],\n",
              " ['<0x86>', 137],\n",
              " ['<0x87>', 138],\n",
              " ['<0x88>', 139],\n",
              " ['<0x89>', 140],\n",
              " ['<0x8A>', 141],\n",
              " ['<0x8B>', 142],\n",
              " ['<0x8C>', 143],\n",
              " ['<0x8D>', 144],\n",
              " ['<0x8E>', 145],\n",
              " ['<0x8F>', 146],\n",
              " ['<0x90>', 147],\n",
              " ['<0x91>', 148],\n",
              " ['<0x92>', 149],\n",
              " ['<0x93>', 150],\n",
              " ['<0x94>', 151],\n",
              " ['<0x95>', 152],\n",
              " ['<0x96>', 153],\n",
              " ['<0x97>', 154],\n",
              " ['<0x98>', 155],\n",
              " ['<0x99>', 156],\n",
              " ['<0x9A>', 157],\n",
              " ['<0x9B>', 158],\n",
              " ['<0x9C>', 159],\n",
              " ['<0x9D>', 160],\n",
              " ['<0x9E>', 161],\n",
              " ['<0x9F>', 162],\n",
              " ['<0xA0>', 163],\n",
              " ['<0xA1>', 164],\n",
              " ['<0xA2>', 165],\n",
              " ['<0xA3>', 166],\n",
              " ['<0xA4>', 167],\n",
              " ['<0xA5>', 168],\n",
              " ['<0xA6>', 169],\n",
              " ['<0xA7>', 170],\n",
              " ['<0xA8>', 171],\n",
              " ['<0xA9>', 172],\n",
              " ['<0xAA>', 173],\n",
              " ['<0xAB>', 174],\n",
              " ['<0xAC>', 175],\n",
              " ['<0xAD>', 176],\n",
              " ['<0xAE>', 177],\n",
              " ['<0xAF>', 178],\n",
              " ['<0xB0>', 179],\n",
              " ['<0xB1>', 180],\n",
              " ['<0xB2>', 181],\n",
              " ['<0xB3>', 182],\n",
              " ['<0xB4>', 183],\n",
              " ['<0xB5>', 184],\n",
              " ['<0xB6>', 185],\n",
              " ['<0xB7>', 186],\n",
              " ['<0xB8>', 187],\n",
              " ['<0xB9>', 188],\n",
              " ['<0xBA>', 189],\n",
              " ['<0xBB>', 190],\n",
              " ['<0xBC>', 191],\n",
              " ['<0xBD>', 192],\n",
              " ['<0xBE>', 193],\n",
              " ['<0xBF>', 194],\n",
              " ['<0xC0>', 195],\n",
              " ['<0xC1>', 196],\n",
              " ['<0xC2>', 197],\n",
              " ['<0xC3>', 198],\n",
              " ['<0xC4>', 199],\n",
              " ['<0xC5>', 200],\n",
              " ['<0xC6>', 201],\n",
              " ['<0xC7>', 202],\n",
              " ['<0xC8>', 203],\n",
              " ['<0xC9>', 204],\n",
              " ['<0xCA>', 205],\n",
              " ['<0xCB>', 206],\n",
              " ['<0xCC>', 207],\n",
              " ['<0xCD>', 208],\n",
              " ['<0xCE>', 209],\n",
              " ['<0xCF>', 210],\n",
              " ['<0xD0>', 211],\n",
              " ['<0xD1>', 212],\n",
              " ['<0xD2>', 213],\n",
              " ['<0xD3>', 214],\n",
              " ['<0xD4>', 215],\n",
              " ['<0xD5>', 216],\n",
              " ['<0xD6>', 217],\n",
              " ['<0xD7>', 218],\n",
              " ['<0xD8>', 219],\n",
              " ['<0xD9>', 220],\n",
              " ['<0xDA>', 221],\n",
              " ['<0xDB>', 222],\n",
              " ['<0xDC>', 223],\n",
              " ['<0xDD>', 224],\n",
              " ['<0xDE>', 225],\n",
              " ['<0xDF>', 226],\n",
              " ['<0xE0>', 227],\n",
              " ['<0xE1>', 228],\n",
              " ['<0xE2>', 229],\n",
              " ['<0xE3>', 230],\n",
              " ['<0xE4>', 231],\n",
              " ['<0xE5>', 232],\n",
              " ['<0xE6>', 233],\n",
              " ['<0xE7>', 234],\n",
              " ['<0xE8>', 235],\n",
              " ['<0xE9>', 236],\n",
              " ['<0xEA>', 237],\n",
              " ['<0xEB>', 238],\n",
              " ['<0xEC>', 239],\n",
              " ['<0xED>', 240],\n",
              " ['<0xEE>', 241],\n",
              " ['<0xEF>', 242],\n",
              " ['<0xF0>', 243],\n",
              " ['<0xF1>', 244],\n",
              " ['<0xF2>', 245],\n",
              " ['<0xF3>', 246],\n",
              " ['<0xF4>', 247],\n",
              " ['<0xF5>', 248],\n",
              " ['<0xF6>', 249],\n",
              " ['<0xF7>', 250],\n",
              " ['<0xF8>', 251],\n",
              " ['<0xF9>', 252],\n",
              " ['<0xFA>', 253],\n",
              " ['<0xFB>', 254],\n",
              " ['<0xFC>', 255],\n",
              " ['<0xFD>', 256],\n",
              " ['<0xFE>', 257],\n",
              " ['<0xFF>', 258],\n",
              " ['en', 259],\n",
              " ['▁t', 260],\n",
              " ['ce', 261],\n",
              " ['in', 262],\n",
              " ['ra', 263],\n",
              " ['▁a', 264],\n",
              " ['de', 265],\n",
              " ['er', 266],\n",
              " ['▁s', 267],\n",
              " ['ent', 268],\n",
              " ['or', 269],\n",
              " ['pr', 270],\n",
              " ['▁m', 271],\n",
              " ['▁u', 272],\n",
              " ['ing', 273],\n",
              " ['▁th', 274],\n",
              " ['ence', 275],\n",
              " ['entence', 276],\n",
              " ['Pi', 277],\n",
              " ['ed', 278],\n",
              " ['em', 279],\n",
              " ['ex', 280],\n",
              " ['is', 281],\n",
              " ['iz', 282],\n",
              " ['la', 283],\n",
              " ['on', 284],\n",
              " ['st', 285],\n",
              " ['▁S', 286],\n",
              " ['Pie', 287],\n",
              " ['end', 288],\n",
              " ['ext', 289],\n",
              " ['▁an', 290],\n",
              " ['▁pr', 291],\n",
              " ['▁to', 292],\n",
              " ['▁un', 293],\n",
              " ['▁the', 294],\n",
              " ['Piece', 295],\n",
              " ['▁Sentence', 296],\n",
              " ['▁SentencePiece', 297],\n",
              " ['.]', 298],\n",
              " ['Ne', 299],\n",
              " ['ag', 300],\n",
              " ['do', 301],\n",
              " ['ec', 302],\n",
              " ['gu', 303],\n",
              " ['ic', 304],\n",
              " ['ir', 305],\n",
              " ['it', 306],\n",
              " ['ly', 307],\n",
              " ['to', 308],\n",
              " ['▁(', 309],\n",
              " ['▁[', 310],\n",
              " ['▁f', 311],\n",
              " ['▁n', 312],\n",
              " ['▁w', 313],\n",
              " ['.])', 314],\n",
              " ['age', 315],\n",
              " ['del', 316],\n",
              " ['ion', 317],\n",
              " ['ken', 318],\n",
              " ['lan', 319],\n",
              " ['ral', 320],\n",
              " ['wor', 321],\n",
              " ['yst', 322],\n",
              " ['▁Ne', 323],\n",
              " ['▁al', 324],\n",
              " ['▁de', 325],\n",
              " ['▁is', 326],\n",
              " ['▁ma', 327],\n",
              " ['▁mo', 328],\n",
              " ['izer', 329],\n",
              " ['rain', 330],\n",
              " ['ural', 331],\n",
              " ['▁and', 332],\n",
              " ['▁lan', 333],\n",
              " ['▁pre', 334],\n",
              " ['guage', 335],\n",
              " ['ystem', 336],\n",
              " ['▁text', 337],\n",
              " ['▁model', 338],\n",
              " ['▁train', 339],\n",
              " ['kenizer', 340],\n",
              " ['▁system', 341],\n",
              " ['▁language', 342],\n",
              " ['▁training', 343],\n",
              " ['.,', 344],\n",
              " ['BP', 345],\n",
              " ['Ku', 346],\n",
              " ['ab', 347],\n",
              " ['as', 348],\n",
              " ['at', 349],\n",
              " ['by', 350],\n",
              " ['co', 351],\n",
              " ['es', 352],\n",
              " ['et', 353],\n",
              " ['if', 354],\n",
              " ['ig', 355],\n",
              " ['im', 356],\n",
              " ['ke', 357],\n",
              " ['lo', 358],\n",
              " ['nr', 359],\n",
              " ['oc', 360],\n",
              " ['e', 361],\n",
              " ['▁', 362],\n",
              " ['n', 363],\n",
              " ['t', 364],\n",
              " ['i', 365],\n",
              " ['r', 366],\n",
              " ['a', 367],\n",
              " ['o', 368],\n",
              " ['s', 369],\n",
              " ['d', 370],\n",
              " ['c', 371],\n",
              " ['l', 372],\n",
              " ['u', 373],\n",
              " ['g', 374],\n",
              " ['m', 375],\n",
              " ['p', 376],\n",
              " ['.', 377],\n",
              " ['h', 378],\n",
              " ['-', 379],\n",
              " ['w', 380],\n",
              " ['y', 381],\n",
              " ['P', 382],\n",
              " ['S', 383],\n",
              " ['b', 384],\n",
              " ['f', 385],\n",
              " ['k', 386],\n",
              " [')', 387],\n",
              " ['x', 388],\n",
              " ['z', 389],\n",
              " ['(', 390],\n",
              " ['N', 391],\n",
              " ['[', 392],\n",
              " [']', 393],\n",
              " ['v', 394],\n",
              " [',', 395],\n",
              " ['/', 396],\n",
              " ['B', 397],\n",
              " ['E', 398],\n",
              " ['K', 399]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = sp.encode(\"hello 안녕하세요\")\n",
        "print([sp.id_to_piece(idx) for idx in ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJtyQuj08dWP",
        "outputId": "56031b37-1064-4fc9-9da0-140cfa3d6ef2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'h', 'e', 'l', 'lo', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
          ]
        }
      ]
    }
  ]
}